{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama using llama-cpp library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 23.3.1 from /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages/pip (python 3.9)\n",
      "Collecting llama-cpp-python==0.1.78\n",
      "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l  Running command pip subprocess to install build dependencies\n",
      "  Collecting setuptools>=42\n",
      "    Using cached setuptools-69.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "  Collecting scikit-build>=0.13\n",
      "    Using cached scikit_build-0.17.6-py3-none-any.whl.metadata (14 kB)\n",
      "  Collecting cmake>=3.18\n",
      "    Using cached cmake-3.29.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
      "  Collecting ninja\n",
      "    Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "  Collecting distro (from scikit-build>=0.13)\n",
      "    Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "  Collecting packaging (from scikit-build>=0.13)\n",
      "    Using cached packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Collecting tomli (from scikit-build>=0.13)\n",
      "    Using cached tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n",
      "    Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "  Using cached setuptools-69.2.0-py3-none-any.whl (821 kB)\n",
      "  Using cached scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
      "  Using cached cmake-3.29.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "  Using cached packaging-24.0-py3-none-any.whl (53 kB)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "  Installing collected packages: ninja, wheel, tomli, setuptools, packaging, distro, cmake, scikit-build\n",
      "  \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "  langchain-core 0.1.32 requires packaging<24.0,>=23.2, but you have packaging 24.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "  \u001b[0mSuccessfully installed cmake-3.29.0.1 distro-1.9.0 ninja-1.11.1.1 packaging-24.0 scikit-build-0.17.6 setuptools-69.2.0 tomli-2.0.1 wheel-0.43.0\n",
      "\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l  Running command Getting requirements to build wheel\n",
      "  running egg_info\n",
      "  writing llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
      "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l  Running command Preparing metadata (pyproject.toml)\n",
      "  running dist_info\n",
      "  creating /tmp/pip-modern-metadata-vuwrt_rd/llama_cpp_python.egg-info\n",
      "  writing /tmp/pip-modern-metadata-vuwrt_rd/llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to /tmp/pip-modern-metadata-vuwrt_rd/llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to /tmp/pip-modern-metadata-vuwrt_rd/llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to /tmp/pip-modern-metadata-vuwrt_rd/llama_cpp_python.egg-info/top_level.txt\n",
      "  writing manifest file '/tmp/pip-modern-metadata-vuwrt_rd/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  reading manifest file '/tmp/pip-modern-metadata-vuwrt_rd/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file '/tmp/pip-modern-metadata-vuwrt_rd/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  creating '/tmp/pip-modern-metadata-vuwrt_rd/llama_cpp_python-0.1.78.dist-info'\n",
      "\u001b[?25hdone\n",
      "Collecting numpy==1.23.4\n",
      "  Obtaining dependency information for numpy==1.23.4 from https://files.pythonhosted.org/packages/92/49/127e40f3f468a9bbce1fe69e97d0f0db524621960e0adc51a977298eb49c/numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n",
      "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/f9/de/dc04a3ea60b22624b51c703a84bbe0184abcd1d0b9bc8074b5d6b7ab90bb/typing_extensions-4.10.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "\n",
      "\n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  \u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.5 will be removed from a future version of\n",
      "    CMake.\n",
      "\n",
      "    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "    CMake that the project does not need compatibility with older versions.\n",
      "\n",
      "  \u001b[0mNot searching for unused variables given on the command line.\n",
      "\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Configuring done (0.3s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1/_cmake_test_compile/build\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja' generator - success\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "  Configuring Project\n",
      "    Working directory:\n",
      "      /tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1/_skbuild/linux-x86_64-3.9/cmake-build\n",
      "    Command:\n",
      "      /tmp/pip-build-env-4im9kkf5/overlay/lib/python3.9/site-packages/cmake/data/bin/cmake /tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1 -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-4im9kkf5/overlay/lib/python3.9/site-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1/_skbuild/linux-x86_64-3.9/cmake-install -DPYTHON_VERSION_STRING:STRING=3.9.18 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-4im9kkf5/overlay/lib/python3.9/site-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/home/dephinate/miniconda3/envs/medicalChatBot/bin/python -DPYTHON_INCLUDE_DIR:PATH=/home/dephinate/miniconda3/envs/medicalChatBot/include/python3.9 -DPYTHON_LIBRARY:PATH=/home/dephinate/miniconda3/envs/medicalChatBot/lib/libpython3.9.so -DPython_EXECUTABLE:PATH=/home/dephinate/miniconda3/envs/medicalChatBot/bin/python -DPython_ROOT_DIR:PATH=/home/dephinate/miniconda3/envs/medicalChatBot -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/home/dephinate/miniconda3/envs/medicalChatBot/include/python3.9 -DPython3_EXECUTABLE:PATH=/home/dephinate/miniconda3/envs/medicalChatBot/bin/python -DPython3_ROOT_DIR:PATH=/home/dephinate/miniconda3/envs/medicalChatBot -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/home/dephinate/miniconda3/envs/medicalChatBot/include/python3.9 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-4im9kkf5/overlay/lib/python3.9/site-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
      "\n",
      "  Not searching for unused variables given on the command line.\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  \u001b[33mCMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n",
      "    Git repository not found; to enable automatic generation of build info,\n",
      "    make sure Git is installed and the project is a Git repository.\n",
      "\n",
      "  \u001b[0m\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Found CUDAToolkit: /usr/include (found version \"11.5.119\")\n",
      "  -- cuBLAS found\n",
      "  -- The CUDA compiler identification is NVIDIA 11.5.119\n",
      "  -- Detecting CUDA compiler ABI info\n",
      "  -- Detecting CUDA compiler ABI info - done\n",
      "  -- Check for working CUDA compiler: /usr/bin/nvcc - skipped\n",
      "  -- Detecting CUDA compile features\n",
      "  -- Detecting CUDA compile features - done\n",
      "  -- Using CUDA architectures: 52;61;70\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "  -- x86 detected\n",
      "  -- Configuring done (1.8s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1/_skbuild/linux-x86_64-3.9/cmake-build\n",
      "  [1/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
      "  [2/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n",
      "  [3/9] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
      "  [4/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
      "  [5/9] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
      "  [6/9] Linking CUDA static library vendor/llama.cpp/libggml_static.a\n",
      "  [7/9] Linking CUDA shared library vendor/llama.cpp/libggml_shared.so\n",
      "  [8/9] Linking CXX shared library vendor/llama.cpp/libllama.so\n",
      "  [8/9] Install the project...\n",
      "  -- Install configuration: \"Release\"\n",
      "  -- Installing: /tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1/_skbuild/linux-x86_64-3.9/cmake-install/lib/libggml_shared.so\n",
      "  -- Installing: /tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1/_skbuild/linux-x86_64-3.9/cmake-install/lib/libllama.so\n",
      "  -- Installing: /tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1/_skbuild/linux-x86_64-3.9/cmake-install/bin/convert.py\n",
      "  -- Installing: /tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1/_skbuild/linux-x86_64-3.9/cmake-install/bin/convert-lora-to-ggml.py\n",
      "  -- Installing: /tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1/_skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/libllama.so\n",
      "\n",
      "  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama.py\n",
      "  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_grammar.py\n",
      "  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_cpp.py\n",
      "  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_types.py\n",
      "  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/__init__.py\n",
      "  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/utils.py\n",
      "  creating directory _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server\n",
      "  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/app.py\n",
      "  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__main__.py\n",
      "  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__init__.py\n",
      "  copying /tmp/pip-install-8n9c4b8h/llama-cpp-python_8a482ff637864fae87290586f969ddc1/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/py.typed\n",
      "\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server\n",
      "  copied 9 files\n",
      "  running build_ext\n",
      "  installing to _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.9/setuptools/lib.linux-x86_64-cpython-39/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copied 11 files\n",
      "  running install_data\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  copying _skbuild/linux-x86_64-3.9/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  writing llama_cpp_python.egg-info/PKG-INFO\n",
      "  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n",
      "  writing requirements to llama_cpp_python.egg-info/requires.txt\n",
      "  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n",
      "  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.9.egg-info\n",
      "  running install_scripts\n",
      "  copied 0 files\n",
      "  creating _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL\n",
      "  creating '/tmp/pip-wheel-i6llazq8/.tmp-0g43o80c/llama_cpp_python-0.1.78-cp39-cp39-linux_x86_64.whl' and adding '_skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel' to it\n",
      "  adding 'llama_cpp/__init__.py'\n",
      "  adding 'llama_cpp/libllama.so'\n",
      "  adding 'llama_cpp/llama.py'\n",
      "  adding 'llama_cpp/llama_cpp.py'\n",
      "  adding 'llama_cpp/llama_grammar.py'\n",
      "  adding 'llama_cpp/llama_types.py'\n",
      "  adding 'llama_cpp/py.typed'\n",
      "  adding 'llama_cpp/utils.py'\n",
      "  adding 'llama_cpp/server/__init__.py'\n",
      "  adding 'llama_cpp/server/__main__.py'\n",
      "  adding 'llama_cpp/server/app.py'\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py'\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/bin/convert.py'\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so'\n",
      "  adding 'llama_cpp_python-0.1.78.data/data/lib/libllama.so'\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/LICENSE.md'\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/METADATA'\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/WHEEL'\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/top_level.txt'\n",
      "  adding 'llama_cpp_python-0.1.78.dist-info/RECORD'\n",
      "  removing _skbuild/linux-x86_64-3.9/setuptools/bdist.linux-x86_64/wheel\n",
      "\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp39-cp39-linux_x86_64.whl size=5837305 sha256=469f0b42a15ab431ea5731130367382e4836178254cf8f5f3eef3ccb746cc185\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vqtt8ug0/wheels/0c/b7/c5/44bc04b1032871ad214d9e23594bbffeccff9278f98691d8b3\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
      "  changing mode of /home/dephinate/miniconda3/envs/medicalChatBot/bin/f2py to 775\n",
      "  changing mode of /home/dephinate/miniconda3/envs/medicalChatBot/bin/f2py3 to 775\n",
      "  changing mode of /home/dephinate/miniconda3/envs/medicalChatBot/bin/f2py3.9 to 775\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.10.0\n",
      "Requirement already satisfied: huggingface_hub in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (0.21.4)\n",
      "Requirement already satisfied: filelock in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from huggingface_hub) (2024.3.0)\n",
      "Requirement already satisfied: requests in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from huggingface_hub) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Collecting llama-cpp-python==0.1.78\n",
      "  Using cached llama_cpp_python-0.1.78-cp39-cp39-linux_x86_64.whl\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python==0.1.78)\n",
      "  Using cached numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.10.0\n",
      "Requirement already satisfied: numpy==1.23.4 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (1.23.4)\n",
      "Requirement already satisfied: accelerate in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from accelerate) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from accelerate) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2024.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
      "Requirement already satisfied: requests in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/dephinate/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# GPU llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install -I llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "!pip install huggingface_hub\n",
    "!pip install -I llama-cpp-python==0.1.78\n",
    "!pip install numpy==1.23.4\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  To Download the model\n",
    "# model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/dephinate/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a linear regression code\"\n",
    "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {prompt}\n",
    "\n",
    "ASSISTANT:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response\u001b[38;5;241m=\u001b[39m\u001b[43mlcpp_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages/llama_cpp/llama.py:1402\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1358\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     grammar: Optional[LlamaGrammar] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1379\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Completion, Iterator[CompletionChunk]]:\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m \n\u001b[1;32m   1382\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages/llama_cpp/llama.py:1353\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1351\u001b[0m     chunks: Iterator[CompletionChunk] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1353\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages/llama_cpp/llama.py:944\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m    942\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 944\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    945\u001b[0m     prompt_tokens,\n\u001b[1;32m    946\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    947\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    948\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    949\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m    950\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m    951\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m    952\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m    953\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m    954\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m    955\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    956\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m    957\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m    958\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m    959\u001b[0m ):\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[1;32m    961\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages/llama_cpp/llama.py:764\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    761\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 764\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    766\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    767\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    777\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m    778\u001b[0m     )\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    781\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages/llama_cpp/llama.py:483\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    481\u001b[0m n_past \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids))\n\u001b[1;32m    482\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m--> 483\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_eval returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot/lib/python3.9/site-packages/llama_cpp/llama_cpp.py:712\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_eval\u001b[39m(\n\u001b[1;32m    706\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    707\u001b[0m     tokens,  \u001b[38;5;66;03m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m     n_threads: c_int,\n\u001b[1;32m    711\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  echo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama using Langchain\n",
    "* Use 7B parameter model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to huggingface account\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import huggingface_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline  = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    trust_remote_code = True,\n",
    "    device_map = \"auto\",\n",
    "    max_length = 1000,\n",
    "    do_sample = True,\n",
    "    top_k = 10,\n",
    "    num_return_sequences = 1, # For chatbots, determines how many alternative sequences you want the model to generate. \n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = huggingface_pipeline(pipeline= pipeline, model_kwargs={'temperature':0})\n",
    "prompt = \"Is God real, don not think about hurting sentiments, give honest answer\"\n",
    "print(llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates    \n",
    "When we are making a user application, it is not ideal to send the user input directly to the llm. Instead we use prompt templates.\n",
    "LangChain facilitates prompt management and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give summary of the book Stillness is the key'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables = [\"book\"],\n",
    "    template = \"Give summary of the book {book}\"\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(book=\"Stillness is the key\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt= prompt)\n",
    "response = chain.run(\"Wings of Fire\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
