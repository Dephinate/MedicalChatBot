{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dephinate/ASU/DL/MedicalChatBot\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GPU llama-cpp-python\n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip -I install llama-cpp-python numpy --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "# !pip install huggingface_hub\n",
    "# !pip install -I llama-cpp-python\n",
    "# !pip install numpy\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ctransformers\n",
    "# !pip install sentence-transformers\n",
    "# !pip install pinecone-client\n",
    "# !pip install langchain\n",
    "# !pip install langchain_pinecone\n",
    "# !pip install flask\n",
    "# !pip install pypdf\n",
    "# !pip install ctransformers[cuda]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries    \n",
    "we need the following:  \n",
    "* something for document data\n",
    "* something for splitting\n",
    "* something for embeddings\n",
    "* something for vectior indexing\n",
    "* something for prompts\n",
    "* something for llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dephinate/miniconda3/envs/medicalChatBot_recent/lib/python3.9/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Loader\n",
    "import langchain\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "import pypdf\n",
    "\n",
    "# Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Vector Store\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Prompts\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Models\n",
    "from langchain.llms import CTransformers\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Chains\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the pdf\n",
    "def load_pdf(path:Path):\n",
    "    loader = DirectoryLoader(   # To load all pdfs from a directory\n",
    "        path=path,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader,\n",
    "        show_progress=True\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.66s/it]\n"
     ]
    }
   ],
   "source": [
    "extracted_data = load_pdf(\"./data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters: chunk size and chunk overlap\n",
    "def text_split(chunk_size:int, chunk_overlap:int, extracted_data:None):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap, separators=['\\n\\n', '\\n', '.', ','])\n",
    "    chunks = splitter.split_documents(extracted_data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_split(chunk_size=500,chunk_overlap=50,extracted_data=extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fucntion to download hugging face embeddings\n",
    "def download_embeddings_from_huggingface(model_name:str):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = download_embeddings_from_huggingface(model_name=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research import PINECONE_API_ENV, PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.07166,\n",
       " 'namespaces': {'medicalChatBot': {'vector_count': 7166}},\n",
       " 'total_vector_count': 7166}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize index\n",
    "pc = Pinecone(\n",
    "    api_key = PINECONE_API_KEY,\n",
    ")\n",
    "pc.list_indexes().names() # just to doublecheck if I am able to connect to my Index\n",
    "index_name = \"medical-chatbot\"\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean DB\n",
    "# index.delete(delete_all=True,namespace=\"medicalChatBot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vectorstore\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embeddings,\n",
    "    namespace=\"medicalChatBot\",\n",
    "    index_name='medical-chatbot'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add records\n",
    "# vectorstore.add_texts(texts=[t.page_content for t in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.07166,\n",
       " 'namespaces': {'medicalChatBot': {'vector_count': 7166}},\n",
       " 'total_vector_count': 7166}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check DB Status\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    numbered_docs = []\n",
    "    for i, doc in enumerate(docs, start=1):  # Enumerate starting from 1\n",
    "        numbered_docs.append(f\"{i}. {doc.page_content}\")  # Add number and content\n",
    "    return \"\\n\".join(numbered_docs)  # Join with newlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'str'>\n",
      "Len:  14\n",
      "Chunks retrieved: \n",
      " 1. .\n",
      "2. .\n",
      "3. .\n"
     ]
    }
   ],
   "source": [
    "# Similarity search example\n",
    "query = \"What?\"\n",
    "\n",
    "chunks_retrieved = vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")\n",
    "chunks_retrieved_formatted = format_docs(chunks_retrieved)\n",
    "print(\"Type: \",type(chunks_retrieved_formatted))\n",
    "print(\"Len: \",len(chunks_retrieved_formatted))\n",
    "print(\"Chunks retrieved: \\n\",chunks_retrieved_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Nancy J. Nordenson\\nAcid reflux seeHeartburn\\nAcidosis seeRespiratory acidosis; Renal\\ntubular acidosis; Metabolic acidosis\\nAcne\\nDefinition\\nAcne is a common skin disease characterized by\\npimples on the face, chest, and back. It occurs when thepores of the skin become clogged with oil, dead skincells, and bacteria.\\nDescription\\nAcne vulgaris, the medical term for common acne, is'),\n",
       " Document(page_content='The goal of treating moderate acne is to decrease\\ninflammation and prevent new comedone formation. Oneeffective treatment is topical tretinoin along with a topical\\nGALE ENCYCLOPEDIA OF MEDICINE 2 25Acne\\nAcne vulgaris affecting a woman’s face. Acne is the general\\nname given to a skin disorder in which the sebaceousglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)GEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25'),\n",
       " Document(page_content='ent purposes. For example, lotions, soaps, gels, andcreams containing benzoyl peroxide or tretinoin may beused to clear up mild to moderately severe acne.Isotretinoin (Accutane) is prescribed only for verysevere, disfiguring acne.\\nAcne is a skin condition that occurs when pores or')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CTransformers\n",
    "llm=CTransformers(model=\"llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4560.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    98.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "# Using LlamaCPP\n",
    "lcpp_llm = None\n",
    "lcpp_llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=32,\n",
    "    n_batch=512,\n",
    "    n_ctx=1024,\n",
    "    f16_kv=True, \n",
    "    temperature = 0.8\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template String\n",
    "template1=\"\"\"\n",
    "[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "Use the Context to answer the Question.\n",
    "Context: {context}\n",
    "Question: {question}[/INST]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2=\"\"\"\n",
    "Answer the question based on the context below.\n",
    "If you don't know the answer, only return I don't know, Do not make an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template3 = \"\"\"\n",
    "<<USER>> {question}\n",
    "\n",
    "<<SYS>>\n",
    "Answer the question based on the context below..\n",
    "{context}\n",
    "system_prompt: \"You are a helpful assistant, below is ... llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most effective Template String\n",
    "template4=\"\"\"\n",
    "[INST] <<SYS>>\n",
    "Answer the query based on the information provided to you. If you don't know the answer, return 'I do not know', Do not make an answer.\n",
    "<</SYS>>\n",
    "query:{question}\n",
    "information:{context}[/INST]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most effective Template String\n",
    "template5=\"\"\"\n",
    "[INST] <<SYS>>\n",
    "Answer the query based on the information. If you don't know the answer, return 'I do not know', Do not make an answer.\n",
    "<</SYS>>\n",
    "query:{question}\n",
    "information:{context}[/INST]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template=\"\\n[INST] <<SYS>>\\nAnswer the query based on the information. If you don't know the answer, return 'I do not know', Do not make an answer.\\n<</SYS>>\\nquery:{question}\\ninformation:{context}[/INST]\\n\\n\")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(template5)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain Chain      \n",
    "1) Method 1: Using RetrievalQA  \n",
    "2) Method 2: Usimg Pipe operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=lcpp_llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': 3}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   23373.24 ms\n",
      "llama_print_timings:      sample time =      37.75 ms /   202 runs   (    0.19 ms per token,  5350.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14461.66 ms /   280 tokens (   51.65 ms per token,    19.36 tokens per second)\n",
      "llama_print_timings:        eval time =   60924.87 ms /   201 runs   (  303.11 ms per token,     3.30 tokens per second)\n",
      "llama_print_timings:       total time =   75849.06 ms /   481 tokens\n"
     ]
    }
   ],
   "source": [
    "response = qa.invoke({\"query\": \"What is furniture?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='suitable surfaces. The space can be as simple as a kitchen\\nor office table, or as fancy as a specialized artist’s studio.\\nThe artist should have adequate time to become'),\n",
       " Document(page_content='stances that may predispose an individual to BED includeheredity and affective disorders, such as major depression.BED patients are also more likely to have a comorbid, orco-existing, diagnosis of impulsive behaviors (for exam-ple, compulsive buying), post-traumatic stress disorder\\n(PTSD), panic disorder , or personality disorders .Individuals who develop BED often come from fam-'),\n",
       " Document(page_content='ry is that the world and body are made up of five mainelements: wood, fire, earth, metal, and water. These ele-ments are all interconnected, and each element eithergenerates or controls another element. For instance,water controls fire and earth generates metal. Each organis associated with one of the five elements. The Chinesesystem uses elements and organs to describe and treatconditions')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['source_documents']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Furniture refers to movable objects that are designed to provide functional support or comfort in a particular space, such as a kitchen or office. According to the information provided, the artist should have adequate time to create furniture that suits their needs and preferences. This could include creating simple or elaborate pieces, such as a kitchen table or a specialized artist's studio.\\nBED, or Binge Eating Disorder, is a condition characterized by recurring episodes of binge eating, followed by feelings of guilt or shame. According to the information provided, factors that may predispose an individual to BED include heredity and affective disorders, such as major depression. BED patients are also more likely to have comorbid diagnoses of impulsive behaviors (such as compulsive buying), post-traumatic stress disorder (PTSD), panic disorder, or personality disorders.\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Method 2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_template(template4)\n\u001b[1;32m      3\u001b[0m prompt\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "# Method 2\n",
    "prompt = ChatPromptTemplate.from_template(template4)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = RunnableParallel(\n",
    "    {\"context\": vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) | format_docs, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "chain = retrieval | prompt | lcpp_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval.invoke(\"Caffien?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(retrieval|prompt).invoke(\"what is Acne and its diffent types and treatment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = chain.invoke(\"what is Acne and its diffent types and treatment?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcpp_llm.invoke(\"what is Acne and its diffent types and treatment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicalChatBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
