{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dephinate/ASU/DL/MedicalChatBot/research\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dephinate/ASU/DL/MedicalChatBot\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-box\n",
    "# !pip install ensure\n",
    "# !pip install -e .\n",
    "# !pip install unstructured\n",
    "# !pip install pdf2image\n",
    "# !pip install pdfminer\n",
    "# !pip uninstall pdfminer\n",
    "# !pip install pillow_heif\n",
    "# !pip install opencv-python\n",
    "# !pip install unstructured-inference\n",
    "# !pip install pytesseract\n",
    "# !pip install pikepdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ce80aa7c-c98e-467c-a100-b4b7e6a07c05'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medicalChatBot.config.configurations import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-16 18:54:05,828,INFO,common,created directory at: artifacts]\n"
     ]
    }
   ],
   "source": [
    "configurationManager = ConfigurationManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_config = configurationManager.get_dataloader_config()\n",
    "datasplitter_config = configurationManager.get_datasplitter_config()\n",
    "vectorization_config =  configurationManager.get_vectorization_config()\n",
    "model_config = configurationManager.get_model_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaderConfig(data_path='artifacts/data', file_types='*.pdf')\n",
      "SplitterConfig(chunk_size=500, chunk_overlap=50)\n",
      "VectorizationConfig(encoder_platform='HuggingFace', encoder_name='sentence-transformers/all-MiniLM-L6-v2', model_name='sentence-transformers/all-MiniLM-L6-v2', index_name='medical-chatbot', namespace='medicalChatBot', num_of_documnets=3)\n",
      "ModelConfig(implementation='LlamaCpp', model_path='artifacts/model/llama-2-7b-chat.Q5_K_M.gguf', model_type='llama', n_gpu_layers=32, n_batch=512, n_ctx=1024, f16_kv=True, temperature=0.8, max_new_tokens=512)\n"
     ]
    }
   ],
   "source": [
    "print(dataloader_config)\n",
    "print(datasplitter_config)\n",
    "print(vectorization_config)\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader\n",
    "import pypdf\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from medicalChatBot.entity  import DataLoaderConfig\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self,\n",
    "                 config : DataLoaderConfig)->None :\n",
    "        self.config = config\n",
    "\n",
    "    # Extract data from the pdf\n",
    "    def load_pdf(self):\n",
    "        loader = DirectoryLoader(   # To load all pdfs from a directory\n",
    "            path=self.config.data_path,\n",
    "            glob=self.config.file_types,\n",
    "            loader_cls=PyPDFLoader,\n",
    "            show_progress=True\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader = DataLoader(config=dataloader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  9.00s/it]\n"
     ]
    }
   ],
   "source": [
    "extracted_data = dataLoader.load_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from medicalChatBot.config.configurations import ConfigurationManager\n",
    "from medicalChatBot.entity  import SplitterConfig\n",
    "\n",
    "class Splitter:\n",
    "    def __init__(self,\n",
    "                 config:SplitterConfig) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    # function to impement recursive text splitting \n",
    "    def split_recursive(self, extracted_data:None):\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size = self.config.chunk_size  , chunk_overlap = self.config.chunk_overlap, separators=['\\n\\n', '\\n', '.', ','])\n",
    "        chunks = splitter.split_documents(extracted_data)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = Splitter(config=datasplitter_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = splitter.split_recursive(extracted_data=extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='TheGALE\\nENCYCLOPEDIA\\nofMEDICINE\\nSECOND EDITION', metadata={'source': 'artifacts/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 0}),\n",
       " Document(page_content='TheGALE\\nENCYCLOPEDIA\\nofMEDICINE\\nSECOND EDITION\\nJACQUELINE L. LONGE, EDITOR\\nDEIRDRE S. BLANCHFIELD, ASSOCIATE EDITOR\\nVOLUME\\nC-F2', metadata={'source': 'artifacts/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 1}),\n",
       " Document(page_content='STAFF\\nJacqueline L. Longe, Project Editor\\nDeirdre S. Blanchfield, Associate Editor\\nChristine B. Jeryan, Managing Editor\\nDonna Olendorf, Senior Editor\\nStacey Blachford, Associate Editor\\nKate Kretschmann, Melissa C. McDade, Ryan\\nThomason, Assistant Editors\\nMark Springer, Technical Specialist\\nAndrea Lopeman, Programmer/Analyst\\nBarbara J. Yarrow, Manager, Imaging and Multimedia\\nContent\\nRobyn V . Young, Project Manager, Imaging and\\nMultimedia Content\\nDean Dauphinais, Senior Editor, Imaging and', metadata={'source': 'artifacts/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 2}),\n",
       " Document(page_content='Dean Dauphinais, Senior Editor, Imaging and\\nMultimedia Content\\nKelly A. Quin, Editor, Imaging and Multimedia Content\\nLeitha Etheridge-Sims, Mary K. Grimes, Dave Oblender,\\nImage Catalogers\\nPamela A. Reed, Imaging Coordinator\\nRandy Bassett, Imaging Supervisor\\nRobert Duncan, Senior Imaging Specialist\\nDan Newell, Imaging Specialist\\nChristine O’Bryan, Graphic Specialist\\nMaria Franklin, Permissions Manager\\nMargaret A. Chamberlain, Permissions Specialist\\nMichelle DiMercurio, Senior Art Director', metadata={'source': 'artifacts/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 2}),\n",
       " Document(page_content='Michelle DiMercurio, Senior Art Director\\nMike Logusz, Graphic Artist\\nMary Beth Trimper, Manager, Composition and\\nElectronic Prepress\\nEvi Seoud, Assistant Manager, Composition Purchasing\\nand Electronic Prepress\\nDorothy Maki, Manufacturing Manager\\nWendy Blurton, Senior Manufacturing SpecialistThe GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITIONSince this page cannot legibly accommodate all copyright notices, the\\nacknowledgments constitute an extension of the copyright notice.', metadata={'source': 'artifacts/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 2}),\n",
       " Document(page_content='While every effort has been made to ensure the reliability of the infor-\\nmation presented in this publication, the Gale Group neither guarantees\\nthe accuracy of the data contained herein nor assumes any responsibili-\\nty for errors, omissions or discrepancies. The Gale Group accepts no\\npayment for listing, and inclusion in the publication of any organiza-\\ntion, agency, institution, publication, service, or individual does not\\nimply endorsement of the editor or publisher. Errors brought to the', metadata={'source': 'artifacts/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 2}),\n",
       " Document(page_content='attention of the publisher and verified to the satisfaction of the publish-\\ner will be corrected in future editions.\\nThis book is printed on recycled paper that meets Environmental Pro-\\ntection Agency standards.\\nThe paper used in this publication meets the minimum requirements of\\nAmerican National Standard for Information Sciences-Permanence\\nPaper for Printed Library Materials, ANSI Z39.48-1984.\\nThis publication is a creative work fully protected by all applicable', metadata={'source': 'artifacts/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 2}),\n",
       " Document(page_content='copyright laws, as well as by misappropriation, trade secret, unfair com-\\npetition, and other applicable laws. The authors and editor of this work\\nhave added value to the underlying factual material herein through one\\nor more of the following: unique and original selection, coordination,\\nexpression, arrangement, and classification of the information.\\nGale Group and design is a trademark used herein under license.\\nAll rights to this publication will be vigorously defended.\\nCopyright © 2002', metadata={'source': 'artifacts/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 2}),\n",
       " Document(page_content='Copyright © 2002\\nGale Group\\n27500 Drake Road\\nFarmington Hills, MI 48331-3535\\nAll rights reserved including the right of reproduction in whole or in\\npart in any form.\\nISBN 0-7876-5489-2 (set)\\n0-7876-5490-6 (V ol. 1)\\n0-7876-5491-4 (V ol. 2)\\n0-7876-5492-2 (V ol. 3)\\n0-7876-5493-0 (V ol. 4)\\n0-7876-5494-9 (V ol. 5)\\nPrinted in the United States of America\\n10 9 8 7 6 5 4 3 2 1\\nLibrary of Congress Cataloging-in-Publication Data\\nGale encyclopedia of medicine / Jacqueline L. Longe, editor;', metadata={'source': 'artifacts/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 2}),\n",
       " Document(page_content='Deirdre S. Blanchfield, associate editor — 2nd ed.\\np. cm.\\nIncludes bibliographical references and index.\\nContents: V ol. 1. A-B — v. 2. C-F — v. 3.\\nG-M — v. 4. N-S — v. 5. T-Z.\\nISBN 0-7876-5489-2 (set: hardcover) — ISBN 0-7876-5490-6\\n(vol. 1) — ISBN 0-7876-5491-4 (vol. 2) — ISBN 0-7876-5492-2\\n(vol. 3) — ISBN 0-7876-5493-0 (vol. 4) — ISBN 0-7876-5494-9\\n(vol. 5)\\n1. Internal medicine—Encyclopedias. I. Longe, Jacqueline L. \\nII. Blanchfield, Deirdre S. III. Gale Research Company.\\nRC41.G35 2001', metadata={'source': 'artifacts/data/The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(chunks))\n",
    "chunks[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from medicalChatBot.entity import VectorizationConfig\n",
    "from medicalChatBot.utils.common import load_env\n",
    "\n",
    "\n",
    "\n",
    "class Vectorizer:\n",
    "    def __init__(self,\n",
    "                 config : VectorizationConfig = None) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    def download_embeddings_from_huggingface(self,model_name:str = None):\n",
    "        model_name = model_name or self.config.encoder_name\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        return embeddings\n",
    "    \n",
    "    def create_pinecone_instance(self,env_file_path:str = None):\n",
    "        load_env(env_file_path=env_file_path)\n",
    "        print(\"Key:\",os.getenv('PINECONE_API_KEY'))\n",
    "        pc = Pinecone(\n",
    "            api_key = f\"{os.getenv('PINECONE_API_KEY')}\"\n",
    "        )\n",
    "        return pc\n",
    "    \n",
    "    def check_pinecone_index_status(self,db_instance, index_name = None):\n",
    "        index_name = index_name or self.config.index_name\n",
    "        index = db_instance.Index(index_name)\n",
    "        return index.describe_index_stats()\n",
    "\n",
    "    def create_pinecone_vectorstore_instance(self,db_instance=None,namespace=None,index_name=None,embeddings=None):\n",
    "        index_name = index_name or self.config.index_name\n",
    "        namespace = namespace or self.config.namespace\n",
    "        \n",
    "        vectorstore = PineconeVectorStore(\n",
    "        index=db_instance.Index(index_name),\n",
    "        embedding=embeddings,\n",
    "        namespace=namespace,\n",
    "        index_name=index_name\n",
    "        )\n",
    "        return vectorstore\n",
    "    \n",
    "    def clean_pinecone_db(self, db_instance,index_name:str=None,namespace:str=None):\n",
    "        index_name = index_name or self.config.index_name\n",
    "        namespace = namespace or self.config.namespace\n",
    "        db_instance.Index(index_name).delete(delete_all=True,namespace=namespace)\n",
    "\n",
    "    def add_records_pinecone_db(self,vectorstore_instance, chunks):\n",
    "        vectorstore_instance.add_texts(texts=[t.page_content for t in chunks])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorizationConfig(encoder_platform='HuggingFace', encoder_name='sentence-transformers/all-MiniLM-L6-v2', model_name='sentence-transformers/all-MiniLM-L6-v2', index_name='medical-chatbot', namespace='medicalChatBot', num_of_documnets=3)\n"
     ]
    }
   ],
   "source": [
    "# initialize Vectorizer\n",
    "print(vectorization_config)\n",
    "vectorizer = Vectorizer(config=vectorization_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-15 04:00:48,716,INFO,SentenceTransformer,Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2]\n",
      "[2024-04-15 04:00:49,835,INFO,SentenceTransformer,Use pytorch device_name: cuda]\n",
      "Key: ce80aa7c-c98e-467c-a100-b4b7e6a07c05\n",
      "embeddings : client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n",
      "\n",
      "index_list: {'indexes': [{'dimension': 384,\n",
      "              'host': 'medical-chatbot-v5y6nq5.svc.gcp-starter.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'medical-chatbot',\n",
      "              'spec': {'pod': {'environment': 'gcp-starter',\n",
      "                               'pod_type': 'starter',\n",
      "                               'pods': 1,\n",
      "                               'replicas': 1,\n",
      "                               'shards': 1}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}}]}\n",
      "\n",
      "index_status : {'dimension': 384,\n",
      " 'index_fullness': 0.1108,\n",
      " 'namespaces': {'medicalChatBot': {'vector_count': 11080}},\n",
      " 'total_vector_count': 11080}\n"
     ]
    }
   ],
   "source": [
    "# embeddings\n",
    "embeddings = vectorizer.download_embeddings_from_huggingface()\n",
    "# Instantiate vectordb\n",
    "pc = vectorizer.create_pinecone_instance(env_file_path=\".env\")\n",
    "index_list = pc.list_indexes()\n",
    "index_status = vectorizer.check_pinecone_index_status(db_instance=pc)\n",
    "\n",
    "print(\"embeddings :\",embeddings)\n",
    "print(\"\\nindex_list:\", index_list)\n",
    "print(\"\\nindex_status :\",index_status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorstore and test\n",
    "vector_store = vectorizer.create_pinecone_vectorstore_instance(db_instance= pc,embeddings=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Some chemotherapy drugs cause hair loss, but it is'),\n",
       " Document(page_content='tone and the scalp is cleaned.'),\n",
       " Document(page_content='lowing treatment:'),\n",
       " Document(page_content='Medical treatments include:'),\n",
       " Document(page_content='Many disorders are associated with celiac disease, though the nature of the connection is unclear. One type of epilepsy is linked to celiac disease. Once their celiac disease is successfully treated, a significant number of these patients have fewer or no seizures. Patients with alopecia areata, a condition where hair loss occurs in sharply defined areas, have been shown to have a higher risk of celiac disease than the general population. There appears to be a higher percentage of celiac disease among people with Down syndrome, but the link between the conditions is unknown.')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"Hairfall treatment ?\"\n",
    "\n",
    "chunks_retrieved = vector_store.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=5  # return 3 most relevant docs\n",
    ")\n",
    "chunks_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing cleanup and check status\n",
    "# vectorizer.clean_pinecone_db(db_instance=pc)\n",
    "\n",
    "# index_status = vectorizer.check_pinecone_index_status(db_instance=pc)\n",
    "# index_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing db loading\n",
    "# vectorizer.add_records_pinecone_db(vectorstore_instance=vector_store,chunks=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test status\n",
    "index_status = vectorizer.check_pinecone_index_status(db_instance=pc)\n",
    "index_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from medicalChatBot.entity import ModelConfig\n",
    "\n",
    "class LoadModel:\n",
    "    def __init__(self,\n",
    "                 config:ModelConfig = None) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    def load_model_from_ctransformers(self, model_path = None, model_type=None,max_new_tokens=None,n_ctx:int=None,temperature=None):\n",
    "        model_path = model_path or self.config.model_path\n",
    "        model_type = model_type or self.config.model_type\n",
    "        max_new_tokens = max_new_tokens or self.config.max_new_tokens\n",
    "        context_length = n_ctx or self.config.n_ctx\n",
    "\n",
    "        temperature = temperature or self.config.temperature\n",
    "\n",
    "        print(model_path)\n",
    "        print(model_type)\n",
    "        print(max_new_tokens)\n",
    "        print(temperature)\n",
    "        print(context_length)\n",
    "\n",
    "        llm=CTransformers(model=model_path,\n",
    "                        model_type=model_type,\n",
    "                        config={'max_new_tokens':max_new_tokens,\n",
    "                                'temperature':temperature,\n",
    "                                'context_length': context_length})\n",
    "        \n",
    "        return llm\n",
    "    \n",
    "    def load_model_from_llamacpp(self,model_path:str=None, n_gpu_layers:int=None, n_batch:int=None, n_ctx:int=None, f16_kv:bool=None, temperature:int=None):\n",
    "        model_path = model_path or self.config.model_path \n",
    "        n_gpu_layers = n_gpu_layers or self.config.n_gpu_layers \n",
    "        n_batch = n_batch or self.config.n_batch\n",
    "        n_ctx = n_ctx or self.config.n_ctx\n",
    "        f16_kv = f16_kv or self.config.f16_kv\n",
    "        temperature = temperature or self.config.temperature\n",
    "        \n",
    "        print(model_path)\n",
    "        print(n_gpu_layers)\n",
    "        print(n_batch)\n",
    "        print(n_ctx)\n",
    "        print(f16_kv)\n",
    "        print(temperature)\n",
    "        \n",
    "        lcpp_llm = None\n",
    "        lcpp_llm = LlamaCpp(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=n_gpu_layers,\n",
    "            n_batch=n_batch,\n",
    "            n_ctx=n_ctx,\n",
    "            f16_kv=f16_kv, \n",
    "            temperature = temperature\n",
    "            )\n",
    "        return lcpp_llm\n",
    "    \n",
    "    def load_model(self):\n",
    "        implementation_lower = self.config.implementation.lower()\n",
    "        print(implementation_lower)\n",
    "        if 'ctransformers' in implementation_lower:\n",
    "            llm = self.load_model_from_ctransformers()\n",
    "        elif 'llama' in implementation_lower or 'llamacpp' in implementation_lower:\n",
    "            llm = self.load_model_from_llamacpp()\n",
    "        return llm        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from medicalChatBot.prompts import *\n",
    "\n",
    "\n",
    "class PromptQueryHandler:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def load_prompt(self, template_name: str = None):\n",
    "        # Dynamically import the module containing the prompt templates\n",
    "        prompt_module = import_module(\"medicalChatBot.prompts\")\n",
    "        default_template_name = \"default_template\"\n",
    "\n",
    "        # Check if template_name is None or if the template is not found\n",
    "        if not template_name:\n",
    "            template = getattr(prompt_module, default_template_name)\n",
    "            print(f\"No template name provided. Using default template: '{default_template_name}'.\")\n",
    "        else:\n",
    "            try:\n",
    "                # Get the template from the module based on the template name\n",
    "                template = getattr(prompt_module, template_name)\n",
    "            except AttributeError:\n",
    "                # If template not found, use a default template``\n",
    "                template = getattr(prompt_module, default_template_name)\n",
    "                print(f\"Template '{template_name}' not found. Using default template: '{default_template_name}'.\")\n",
    "\n",
    "        # Create a PromptTemplate object from the retrieved template\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "        return prompt\n",
    "    \n",
    "    def initialize_chain_with_retrievalqa(self,llm,vector_store,return_source_documents:bool,prompt=None,k:int=3):\n",
    "        prompt = prompt or self.load_prompt()\n",
    "        print(\"prompt: \",prompt)\n",
    "        qa=RetrievalQA.from_chain_type(\n",
    "            llm=llm, \n",
    "            chain_type=\"stuff\", \n",
    "            retriever=vector_store.as_retriever(search_kwargs={'k': k}),\n",
    "            return_source_documents=return_source_documents, \n",
    "            chain_type_kwargs={\"prompt\": prompt})\n",
    "        return qa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = configurationManager.get_model_config()\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadModel = LoadModel(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default loader\n",
    "model_default = loadModel.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTransformers loader\n",
    "model_ctran = loadModel.load_model_from_ctransformers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctran.invoke(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LammaCpp loader\n",
    "model_llamacpp = loadModel.load_model_from_llamacpp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llamacpp.invoke(\"How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to treat hairfall?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptQueryHandler = PromptQueryHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CTransfprmers\n",
    "qa_ct = promptQueryHandler.initialize_chain_with_retrievalqa(llm=model_ctran,vector_store=vector_store,return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_ct = qa_ct.invoke({\"query\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LLamaCPP\n",
    "qa_llama = promptQueryHandler.initialize_chain_with_retrievalqa(llm=model_default,vector_store=vector_store,return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_llama = qa_llama.invoke({\"query\":\"What is Caffiene?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_llama['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_llama['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_llama['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai-whisper\n",
    "# !pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dephinate/ASU/DL/MedicalChatBot\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.42G/1.42G [07:13<00:00, 3.52MiB/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 3.80 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 3.77 GiB memory in use. Of the allocated memory 3.67 GiB is allocated by PyTorch, and 10.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Download model to a directory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m whisper_model \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedium\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43martifacts/model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot_recent/lib/python3.9/site-packages/whisper/__init__.py:156\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alignment_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_alignment_heads(alignment_heads)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot_recent/lib/python3.9/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot_recent/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot_recent/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot_recent/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot_recent/lib/python3.9/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot_recent/lib/python3.9/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 3.80 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 3.77 GiB memory in use. Of the allocated memory 3.67 GiB is allocated by PyTorch, and 10.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Download model to a directory\n",
    "whisper_model = whisper.load_model(name=\"medium\",download_root=\"artifacts/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from a directory\n",
    "whisper_model1 = whisper.load_model(name=\"artifacts/model/whisper_base.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 512)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whisper_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe audio data\n",
    "# Let's do this only if we haven't created the transcription file yet.\n",
    "if not os.path.exists(\"artifacts/data/transcription.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "    # Let's load the base model. This is not the most accurate\n",
    "    # model but it's fast.\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicalChatBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
