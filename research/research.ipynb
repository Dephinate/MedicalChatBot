{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dephinate/ASU/DL/MedicalChatBot/research\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dephinate/ASU/DL/MedicalChatBot\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-box\n",
    "# !pip install ensure\n",
    "# !pip install -e .\n",
    "# !pip install unstructured\n",
    "# !pip install pdf2image\n",
    "# !pip install pdfminer\n",
    "# !pip uninstall pdfminer\n",
    "# !pip install pillow_heif\n",
    "# !pip install opencv-python\n",
    "# !pip install unstructured-inference\n",
    "# !pip install pytesseract\n",
    "# !pip install pikepdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ce80aa7c-c98e-467c-a100-b4b7e6a07c05'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print distribution\n",
    "import numpy as np\n",
    "def print_stats(len_list,docs):\n",
    "    len_arr = np.array(len_list)\n",
    "    print(\"num of chunks: \",len(len_arr))\n",
    "    print(\"min: \",np.argmin(len_arr) ,np.min(len_arr))\n",
    "    print(\"max: \",np.argmax(len_arr) ,np.max(len_arr))\n",
    "    print(\"avg :\",np.mean(len_arr))\n",
    "    print(\"std :\",np.std(len_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting\n",
    "import re\n",
    "from unstructured.cleaners.core import clean,group_broken_paragraphs\n",
    "def format_docs(doc):\n",
    "    print(doc.page_content)\n",
    "    para_split_re = re.compile(r\"(\\s*\\n\\s*){3}\")\n",
    "    print(\"\\nRestructured: \",group_broken_paragraphs(doc.page_content,paragraph_split=para_split_re))\n",
    "    print(\"\\nSource: \",doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unstructured.cleaners.core import clean,group_broken_paragraphs\n",
    "para_split_re = re.compile(r\"(\\s*\\n\\s*){3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medicalChatBot.config.configurations import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config/config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m configurationManager \u001b[38;5;241m=\u001b[39m \u001b[43mConfigurationManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ASU/DL/MedicalChatBot/src/medicalChatBot/config/configurations.py:9\u001b[0m, in \u001b[0;36mConfigurationManager.__init__\u001b[0;34m(self, config_filepath, param_filepath)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      7\u001b[0m              config_filepath\u001b[38;5;241m=\u001b[39mCONFIG_FILE_PATH,\n\u001b[1;32m      8\u001b[0m              param_filepath\u001b[38;5;241m=\u001b[39mPARAMS_FILE_PATH) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[43mread_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m read_yaml(param_filepath)\n\u001b[1;32m     12\u001b[0m     create_directory([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39martifacts_root])\n",
      "File \u001b[0;32m~/miniconda3/envs/medicalChatBot_recent/lib/python3.9/site-packages/ensure/main.py:872\u001b[0m, in \u001b[0;36mWrappedFunctionReturn.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;132;01m{arg}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{valt}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match annotation type \u001b[39m\u001b[38;5;132;01m{t}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EnsureError(msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39marg, f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, t\u001b[38;5;241m=\u001b[39mtempl, valt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(value)))\n\u001b[0;32m--> 872\u001b[0m return_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_val, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_templ):\n\u001b[1;32m    874\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn value of \u001b[39m\u001b[38;5;132;01m{f}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{valt}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match annotation type \u001b[39m\u001b[38;5;132;01m{t}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/ASU/DL/MedicalChatBot/src/medicalChatBot/utils/common.py:29\u001b[0m, in \u001b[0;36mread_yaml\u001b[0;34m(path_to_yaml)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/ASU/DL/MedicalChatBot/src/medicalChatBot/utils/common.py:23\u001b[0m, in \u001b[0;36mread_yaml\u001b[0;34m(path_to_yaml)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;129m@ensure_annotations\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_yaml\u001b[39m(path_to_yaml: Path)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConfigBox:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_to_yaml\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m yaml_file:\n\u001b[1;32m     24\u001b[0m             content \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(yaml_file)\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m (ConfigBox(content))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config/config.yaml'"
     ]
    }
   ],
   "source": [
    "configurationManager = ConfigurationManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_config = configurationManager.get_dataloader_config()\n",
    "datasplitter_config = configurationManager.get_datasplitter_config()\n",
    "vectorization_config =  configurationManager.get_vectorization_config()\n",
    "model_config = configurationManager.get_model_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataloader_config)\n",
    "print(datasplitter_config)\n",
    "print(vectorization_config)\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader\n",
    "import pypdf\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from medicalChatBot.entity  import DataLoaderConfig\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self,\n",
    "                 config : DataLoaderConfig)->None :\n",
    "        self.config = config\n",
    "\n",
    "    # Extract data from the pdf\n",
    "    def load_pdf(self):\n",
    "        loader = DirectoryLoader(   # To load all pdfs from a directory\n",
    "            path=self.config.data_path,\n",
    "            glob=self.config.file_types,\n",
    "            loader_cls=PyPDFLoader,\n",
    "            show_progress=True\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoader = DataLoader(config=dataloader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = dataLoader.load_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from medicalChatBot.config.configurations import ConfigurationManager\n",
    "from medicalChatBot.entity  import SplitterConfig\n",
    "\n",
    "class Splitter:\n",
    "    def __init__(self,\n",
    "                 config:SplitterConfig) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    # function to impement recursive text splitting \n",
    "    def split_recursive(self, extracted_data:None):\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size = self.config.chunk_size  , chunk_overlap = self.config.chunk_overlap, separators=['\\n\\n', '\\n', '.', ','])\n",
    "        chunks = splitter.split_documents(extracted_data)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = Splitter(config=datasplitter_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = splitter.split_recursive(extracted_data=extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(chunk.page_content)for chunk in chunks]\n",
    "print_stats(lens,docs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from medicalChatBot.entity import VectorizationConfig\n",
    "from medicalChatBot.utils.common import load_env\n",
    "\n",
    "\n",
    "\n",
    "class Vectorizer:\n",
    "    def __init__(self,\n",
    "                 config : VectorizationConfig = None) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    def download_embeddings_from_huggingface(self,model_name:str = None):\n",
    "        model_name = model_name or self.config.encoder_name\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        return embeddings\n",
    "    \n",
    "    def create_pinecone_instance(self,env_file_path:str = None):\n",
    "        load_env(env_file_path=env_file_path)\n",
    "        print(\"Key:\",os.getenv('PINECONE_API_KEY'))\n",
    "        pc = Pinecone(\n",
    "            api_key = f\"{os.getenv('PINECONE_API_KEY')}\"\n",
    "        )\n",
    "        return pc\n",
    "    \n",
    "    def check_pinecone_index_status(self,db_instance, index_name = None):\n",
    "        index_name = index_name or self.config.index_name\n",
    "        index = db_instance.Index(index_name)\n",
    "        return index.describe_index_stats()\n",
    "\n",
    "    def create_pinecone_vectorstore_instance(self,db_instance=None,namespace=None,index_name=None,embeddings=None):\n",
    "        index_name = index_name or self.config.index_name\n",
    "        namespace = namespace or self.config.namespace\n",
    "        \n",
    "        vectorstore = PineconeVectorStore(\n",
    "        index=db_instance.Index(index_name),\n",
    "        embedding=embeddings,\n",
    "        namespace=namespace,\n",
    "        index_name=index_name\n",
    "        )\n",
    "        return vectorstore\n",
    "    \n",
    "    def clean_pinecone_db(self, db_instance,index_name:str=None,namespace:str=None):\n",
    "        index_name = index_name or self.config.index_name\n",
    "        namespace = namespace or self.config.namespace\n",
    "        db_instance.Index(index_name).delete(delete_all=True,namespace=namespace)\n",
    "\n",
    "    def add_records_pinecone_db(self,vectorstore_instance, chunks):\n",
    "        vectorstore_instance.add_texts(texts=[t.page_content for t in chunks])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Vectorizer\n",
    "print(vectorization_config)\n",
    "vectorizer = Vectorizer(config=vectorization_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "embeddings = vectorizer.download_embeddings_from_huggingface()\n",
    "# Instantiate vectordb\n",
    "pc = vectorizer.create_pinecone_instance(env_file_path=\".env\")\n",
    "index_list = pc.list_indexes()\n",
    "index_status = vectorizer.check_pinecone_index_status(db_instance=pc)\n",
    "\n",
    "print(\"embeddings :\",embeddings)\n",
    "print(\"\\nindex_list:\", index_list)\n",
    "print(\"\\nindex_status :\",index_status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorstore and test\n",
    "vector_store = vectorizer.create_pinecone_vectorstore_instance(db_instance= pc,embeddings=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What is Acne?\"\n",
    "\n",
    "chunks_retrieved = vector_store.similarity_search_with_relevance_scores(\n",
    "    query,  # our search query\n",
    "    k=3,  # return 3 most relevant docs\n",
    "    # fetch_k = 30\n",
    "\n",
    ")\n",
    "chunks_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for chunk in chunks_retrieved:\n",
    "    print(i,\":\",group_broken_paragraphs(chunk[0].page_content,paragraph_split=para_split_re),\"\\n\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What is acne?\"\n",
    "\n",
    "chunks_retrieved = vector_store.max_marginal_relevance_search(\n",
    "    query,  # our search query\n",
    "    k=3,  # return 3 most relevant docs\n",
    "    fetch_k = 30\n",
    "\n",
    ")\n",
    "chunks_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for chunk in chunks_retrieved:\n",
    "    print(i,\":\",group_broken_paragraphs(chunk.page_content,paragraph_split=para_split_re),\"\\n\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing cleanup and check status\n",
    "# vectorizer.clean_pinecone_db(db_instance=pc)\n",
    "\n",
    "# index_status = vectorizer.check_pinecone_index_status(db_instance=pc)\n",
    "# index_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing db loading\n",
    "# vectorizer.add_records_pinecone_db(vectorstore_instance=vector_store,chunks=chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test status\n",
    "index_status = vectorizer.check_pinecone_index_status(db_instance=pc)\n",
    "index_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from medicalChatBot.entity import ModelConfig\n",
    "\n",
    "class LoadModel:\n",
    "    def __init__(self,\n",
    "                 config:ModelConfig = None) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    def load_model_from_ctransformers(self, model_path = None, model_type=None,max_new_tokens=None,n_ctx:int=None,temperature=None):\n",
    "        model_path = model_path or self.config.model_path\n",
    "        model_type = model_type or self.config.model_type\n",
    "        max_new_tokens = max_new_tokens or self.config.max_new_tokens\n",
    "        context_length = n_ctx or self.config.n_ctx\n",
    "\n",
    "        temperature = temperature or self.config.temperature\n",
    "\n",
    "        print(model_path)\n",
    "        print(model_type)\n",
    "        print(max_new_tokens)\n",
    "        print(temperature)\n",
    "        print(context_length)\n",
    "\n",
    "        llm=CTransformers(model=model_path,\n",
    "                        model_type=model_type,\n",
    "                        config={'max_new_tokens':max_new_tokens,\n",
    "                                'temperature':temperature,\n",
    "                                'context_length': context_length})\n",
    "        \n",
    "        return llm\n",
    "    \n",
    "    def load_model_from_llamacpp(self,model_path:str=None, n_gpu_layers:int=None, n_batch:int=None, n_ctx:int=None, f16_kv:bool=None, temperature:int=None):\n",
    "        model_path = model_path or self.config.model_path \n",
    "        n_gpu_layers = n_gpu_layers or self.config.n_gpu_layers \n",
    "        n_batch = n_batch or self.config.n_batch\n",
    "        n_ctx = n_ctx or self.config.n_ctx\n",
    "        f16_kv = f16_kv or self.config.f16_kv\n",
    "        temperature = temperature or self.config.temperature\n",
    "        \n",
    "        print(model_path)\n",
    "        print(n_gpu_layers)\n",
    "        print(n_batch)\n",
    "        print(n_ctx)\n",
    "        print(f16_kv)\n",
    "        print(temperature)\n",
    "        \n",
    "        lcpp_llm = None\n",
    "        lcpp_llm = LlamaCpp(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=n_gpu_layers,\n",
    "            n_batch=n_batch,\n",
    "            n_ctx=n_ctx,\n",
    "            f16_kv=f16_kv, \n",
    "            temperature = temperature\n",
    "            )\n",
    "        return lcpp_llm\n",
    "    \n",
    "    def load_model(self):\n",
    "        implementation_lower = self.config.implementation.lower()\n",
    "        print(implementation_lower)\n",
    "        if 'ctransformers' in implementation_lower:\n",
    "            llm = self.load_model_from_ctransformers()\n",
    "        elif 'llama' in implementation_lower or 'llamacpp' in implementation_lower:\n",
    "            llm = self.load_model_from_llamacpp()\n",
    "        return llm        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = configurationManager.get_model_config()\n",
    "loadModel = LoadModel(config=model_config)\n",
    "# Default loader\n",
    "model_default = loadModel.load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(model_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is acne?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "compressed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import import_module\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from medicalChatBot.prompts import *\n",
    "\n",
    "\n",
    "class PromptQueryHandler:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def load_prompt(self, template_name: str = None):\n",
    "        # Dynamically import the module containing the prompt templates\n",
    "        prompt_module = import_module(\"medicalChatBot.prompts\")\n",
    "        default_template_name = \"default_template\"\n",
    "\n",
    "        # Check if template_name is None or if the template is not found\n",
    "        if not template_name:\n",
    "            template = getattr(prompt_module, default_template_name)\n",
    "            print(f\"No template name provided. Using default template: '{default_template_name}'.\")\n",
    "        else:\n",
    "            try:\n",
    "                # Get the template from the module based on the template name\n",
    "                template = getattr(prompt_module, template_name)\n",
    "            except AttributeError:\n",
    "                # If template not found, use a default template``\n",
    "                template = getattr(prompt_module, default_template_name)\n",
    "                print(f\"Template '{template_name}' not found. Using default template: '{default_template_name}'.\")\n",
    "\n",
    "        # Create a PromptTemplate object from the retrieved template\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "        return prompt\n",
    "    \n",
    "    def initialize_chain_with_retrievalqa(self,llm,vector_store,return_source_documents:bool,prompt=None,k:int=3):\n",
    "        prompt = self.load_prompt(template_name=prompt)\n",
    "        print(\"prompt: \",prompt)\n",
    "        qa=RetrievalQA.from_chain_type(\n",
    "            llm=llm, \n",
    "            chain_type=\"stuff\", \n",
    "            retriever=vector_store.as_retriever(search_kwargs={'k': k}),\n",
    "            return_source_documents=return_source_documents, \n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            verbose = True)\n",
    "            \n",
    "        return qa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = configurationManager.get_model_config()\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadModel = LoadModel(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default loader\n",
    "model_default = loadModel.load_model_from_llamacpp(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTransformers loader\n",
    "model_ctran = loadModel.load_model_from_ctransformers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctran.invoke(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LammaCpp loader\n",
    "model_llamacpp = loadModel.load_model_from_llamacpp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llamacpp.invoke(\"How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to treat hairfall?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptQueryHandler = PromptQueryHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CTransfprmers\n",
    "qa_ct = promptQueryHandler.initialize_chain_with_retrievalqa(llm=model_ctran,vector_store=vector_store,return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_ct = qa_ct.invoke({\"query\":query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LLamaCPP\n",
    "qa_llama = promptQueryHandler.initialize_chain_with_retrievalqa(llm=model_default,vector_store=vector_store,return_source_documents=True,prompt=\"template4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_llama = qa_llama.invoke({\"query\":\"What is Caffiene?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_llama['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_llama['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_default.invoke(\"What is Caffiene?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_llama['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai-whisper\n",
    "# !pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model to a directory\n",
    "whisper_model = whisper.load_model(name=\"medium\",download_root=\"artifacts/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from a directory\n",
    "whisper_model1 = whisper.load_model(name=\"artifacts/model/whisper_base.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whisper_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe audio data\n",
    "# Let's do this only if we haven't created the transcription file yet.\n",
    "if not os.path.exists(\"artifacts/data/transcription.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "    # Let's load the base model. This is not the most accurate\n",
    "    # model but it's fast.\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "\n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicalChatBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
